#!/bin/bash

# Configura√ß√µes do Script
# Sai imediatamente se um comando sair com um status diferente de zero.
set -e # Removido -u para evitar problemas com vari√°veis n√£o definidas em alguns contextos de find/wc
       # pipefail pode ser adicionado se necess√°rio: set -eo pipefail

# --- Configura√ß√£o ---
# Caminho relativo ao script dentro de ferramenta_downloader/
MARKDOWN_FILE="pesquisa/pesquisa.md"
# Ajustado para salvar na pasta pdfs_baixados/ na raiz do projeto
OUTPUT_BASE_DIR_PARENT="../pdfs_baixados" # Pasta pai para os downloads
OUTPUT_DIR_BASENAME="downloads_bash_$(date +%Y%m%d_%H%M%S)"

# Diret√≥rio de sa√≠da completo
OUTPUT_BASE_DIR="${OUTPUT_BASE_DIR_PARENT}/${OUTPUT_DIR_BASENAME}"

# --- Fun√ß√µes Auxiliares ---
log_info() {
    echo "[INFO] $(date +'%Y-%m-%d %H:%M:%S') - $1"
}

log_error() {
    echo "[ERRO] $(date +'%Y-%m-%d %H:%M:%S') - $1" >&2
}

# Fun√ß√£o para sanitizar URL para criar um nome de diret√≥rio seguro
sanitize_url_for_dirname() {
    local s_url="$1"
    echo "$s_url" | \
        sed -e 's|https\?://||' \
            -e 's|www\.||' \
            -e 's|/|_|g' \
            -e 's|[^a-zA-Z0-9_-]||g' | \
        cut -c1-60
}

# Fun√ß√£o para extrair URLs do arquivo markdown (simplificada para Bash)
extract_urls_from_file() {
    local file="$1"
    if [ ! -f "$file" ]; then
        log_error "Arquivo de entrada '$file' n√£o encontrado."
        return 1
    fi
    # Regex simplificada para Bash (grep -Eo para http/https)
    # Tenta pegar de [text](URL), <URL> e URLs soltas
    grep -Eoh \
        -e '\[[^]]*]\((https?://[^)]+)\)' \
        -e '<\s*(https?://[^>]+)\s*>' \
        -e 'https?://[^ <>"`()]+' \
        "$file" | \
    sed -E \
        -e 's/^\[[^]]*]\((.*)\)$/\1/' \
        -e 's/^<\s*(.*)\s*>$/\1/' | \
    sed -E 's/[().,;:]*$//' | \
    grep -E '^https?://' | \
    sort -u
}

# --- Script Principal ---
# Muda para o diret√≥rio do script para que os caminhos relativos funcionem
cd "$(dirname "$0")"

log_info "üöÄ Iniciando script Bash para baixar PDFs..."

# 1. Criar Diret√≥rio de Sa√≠da Principal
mkdir -p "$OUTPUT_BASE_DIR"
if [ ! -d "$OUTPUT_BASE_DIR" ]; then
    log_error "‚ùå Falha ao criar diret√≥rio de sa√≠da: $OUTPUT_BASE_DIR"
    exit 1
fi
log_info "üìÇ Arquivos ser√£o salvos em: $(cd "$OUTPUT_BASE_DIR" && pwd)" # Mostra caminho absoluto

# 2. Verificar se o arquivo markdown de entrada existe
if [ ! -f "$MARKDOWN_FILE" ]; then
    log_error "‚ùå Arquivo de entrada '$MARKDOWN_FILE' n√£o encontrado."
    exit 1
fi

# 3. Extrair URLs do arquivo
extracted_urls=$(extract_urls_from_file "$MARKDOWN_FILE")

if [ -z "$extracted_urls" ]; then
    log_info "‚ÑπÔ∏è Nenhuma URL v√°lida encontrada em '$MARKDOWN_FILE'."
    if [ -d "$OUTPUT_BASE_DIR" ] && [ -z "$(ls -A "$OUTPUT_BASE_DIR")" ]; then
        log_info "üóëÔ∏è Diret√≥rio de sa√≠da principal '$OUTPUT_BASE_DIR' est√° vazio. Removendo."
        rmdir "$OUTPUT_BASE_DIR" 2>/dev/null || true
    fi
    exit 0
fi

log_info "üîé Encontradas $(echo "$extracted_urls" | wc -l | xargs) URLs √∫nicas para processar."

echo "$extracted_urls" | while IFS= read -r url; do
    log_info "----------------------------------------------------------------"
    log_info "üîó Processando URL: $url"
    
    url_sanitized_name=$(sanitize_url_for_dirname "$url")

    # A. VERIFICAR SE √â UM LINK DIRETO PARA PDF
    if echo "$url" | grep -iqE '\.pdf([?#].*)?$'; then
        log_info "üéØ Link direto para PDF detectado."
        target_subdir_for_url="${OUTPUT_BASE_DIR}/${url_sanitized_name}_DIRECTPDF"
        mkdir -p "$target_subdir_for_url"

        log_info "üì• Baixando: $url para $target_subdir_for_url"
        wget_direct_status=0
        wget --content-disposition -P "$target_subdir_for_url" -nc -nv \
             --timeout=30 --tries=2 \
             "$url" || wget_direct_status=$?
        
        if [ $wget_direct_status -eq 0 ]; then
            # Verifica se algum arquivo foi realmente baixado/existe no diret√≥rio
            if find "$target_subdir_for_url" -maxdepth 1 -type f -print -quit 2>/dev/null | grep -q .; then
                log_info "‚úÖ PDF baixado (ou j√° existia) em $target_subdir_for_url."
            else
                log_info "‚ÑπÔ∏è Comando wget para PDF direto executado, mas nenhum arquivo encontrado em $target_subdir_for_url."
            fi
        else
            log_error "‚ö†Ô∏è Falha ao baixar PDF direto (c√≥digo: $wget_direct_status): $url"
            if [ -d "$target_subdir_for_url" ] && [ -z "$(ls -A "$target_subdir_for_url")" ]; then
                 log_info "üóëÔ∏è Diret√≥rio $target_subdir_for_url est√° vazio ap√≥s falha. Removendo."
                 rmdir "$target_subdir_for_url" 2>/dev/null || true
            fi
        fi
    else
        # B. √â UM LINK DE P√ÅGINA: TENTAR ENCONTRAR PDFs DENTRO DELA (usando wget -r como principal)
        log_info "üìÑ Link de p√°gina detectado. Procurando PDFs dentro de: $url"
        
        target_subdir_for_url_wget="${OUTPUT_BASE_DIR}/${url_sanitized_name}_FROMPAGE_WGET"
        mkdir -p "$target_subdir_for_url_wget"
        log_info "üîé Tentativa (wget recursivo) para $url -> salvando em $target_subdir_for_url_wget"
        
        wget_recursive_cmd_status=0
        wget -r -l1 --no-parent -A.pdf --content-disposition -nd -nc -nv \
             --timeout=30 --tries=2 \
             -P "$target_subdir_for_url_wget" "$url" || wget_recursive_cmd_status=$?
        
        total_pdfs_in_page_wget_dir=$(find "$target_subdir_for_url_wget" -maxdepth 1 -type f -iname "*.pdf" 2>/dev/null | wc -l | xargs) # xargs para trim

        if [ "$wget_recursive_cmd_status" -eq 0 ] && [ "$total_pdfs_in_page_wget_dir" -gt 0 ]; then
            log_info "‚úÖ Sucesso (wget recursivo): $total_pdfs_in_page_wget_dir PDF(s) encontrados/baixados de $url."
        else
            if [ "$wget_recursive_cmd_status" -ne 0 ]; then
                log_info "‚ö†Ô∏è Falha no comando wget recursivo (c√≥digo: $wget_recursive_cmd_status) para $url."
            fi
            if [ "$total_pdfs_in_page_wget_dir" -eq 0 ]; then
                 log_info "‚ÑπÔ∏è Nenhum PDF encontrado por 'wget recursivo' em $url."
            fi
            # Opcional: adicionar fallback para lynx aqui se desejado, similar ao script Python.
            # Por simplicidade, este script Bash foca no wget -r para p√°ginas.

            # Limpar diret√≥rio do wget se estiver vazio
            if [ "$total_pdfs_in_page_wget_dir" -eq 0 ] && [ -d "$target_subdir_for_url_wget" ] && [ -z "$(ls -A "$target_subdir_for_url_wget")" ]; then
                log_info "üóëÔ∏è Diret√≥rio wget $target_subdir_for_url_wget est√° vazio. Removendo."
                rmdir "$target_subdir_for_url_wget" 2>/dev/null || true
            fi
        fi
    fi
done <<< "$extracted_urls" # Usar here-string para alimentar o loop while

log_info "----------------------------------------------------------------"
log_info "üéâ Script Bash conclu√≠do! Verifique o diret√≥rio '$(cd "$OUTPUT_BASE_DIR" && pwd)'."
# Limpeza final do diret√≥rio de output base se estiver vazio
if [ -d "$OUTPUT_BASE_DIR" ] && [ -z "$(ls -A "$OUTPUT_BASE_DIR")" ]; then
    log_info "üóëÔ∏è Diret√≥rio de sa√≠da principal '$OUTPUT_BASE_DIR' est√° vazio. Removendo."
    rmdir "$OUTPUT_BASE_DIR" 2>/dev/null || true
fi
